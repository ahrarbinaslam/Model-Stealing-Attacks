{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Necessary Libraries"
      ],
      "metadata": {
        "id": "8Ygn6ogV8DMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx -q\n",
        "!pip install onnxruntime -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeIfpZ-KheoZ",
        "outputId": "08b34a1d-8e20-4cf8-87b6-94aa45598a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m490.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration and Remote Embedding Extraction from Victim API"
      ],
      "metadata": {
        "id": "VkCrrpDo82kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, torch, json, io, base64, pickle, os, time\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Configuration\n",
        "TOKEN = \"93145372\"\n",
        "SEED = \"53027020\"\n",
        "PORT = \"9935\"\n",
        "BATCH_SIZE = 1000\n",
        "SAVE_DIR = \"embedding_batches\"\n",
        "\n",
        "# Load dataset\n",
        "# Load the image dataset stored in a PyTorch .pt file format.\n",
        "# Assumes dataset.imgs is a list of PIL Images.\n",
        "dataset = torch.load(\"ModelStealingPub.pt\", weights_only=False)\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "print(f\"Loaded dataset with {len(dataset.imgs)} images.\")\n",
        "\n",
        "# Victim API Query Function\n",
        "# Sends a batch of base64-encoded PNG images to the model API via HTTP GET.\n",
        "# Returns the corresponding output embeddings (representations).\n",
        "def model_stealing(images, port):\n",
        "    url = f\"http://34.122.51.94:{port}/query\"  # API endpoint\n",
        "    image_data = []\n",
        "\n",
        "    # Convert each image to base64 after saving as PNG in-memory\n",
        "    for img in images:\n",
        "        img_byte_arr = io.BytesIO()\n",
        "        img.save(img_byte_arr, format='PNG')\n",
        "        img_byte_arr.seek(0)\n",
        "        img_base64 = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')\n",
        "        image_data.append(img_base64)\n",
        "\n",
        "    # Send the encoded image list as a JSON string\n",
        "    r = requests.get(url, files={\"file\": json.dumps(image_data)}, headers={\"token\": TOKEN})\n",
        "\n",
        "    # Parse and return output if successful\n",
        "    if r.status_code == 200:\n",
        "        return r.json()[\"representations\"]\n",
        "    else:\n",
        "        # Raise an error if the API returns a failure\n",
        "        raise RuntimeError(f\"Query failed: {r.status_code} - {r.text}\")\n",
        "\n",
        "# Query dataset in 1000-image batches\n",
        "# Iterate over the dataset and query the victim model in chunks of BATCH_SIZE.\n",
        "\n",
        "total_images = len(dataset.imgs)\n",
        "num_batches = (total_images + BATCH_SIZE - 1) // BATCH_SIZE  # Calculate total number of batches\n",
        "\n",
        "for i in range(num_batches):\n",
        "    path = os.path.join(SAVE_DIR, f\"embeddings_batch_{i}.pickle\")\n",
        "\n",
        "    # Skip already processed batches to avoid redundant work\n",
        "    if os.path.exists(path):\n",
        "        print(f\"Skipping batch {i}, already saved.\")\n",
        "        continue\n",
        "\n",
        "    start = i * BATCH_SIZE\n",
        "    end = min(start + BATCH_SIZE, total_images)\n",
        "    print(f\"Querying batch {i+1}/{num_batches} [{start}:{end}]...\")\n",
        "\n",
        "    batch_imgs = dataset.imgs[start:end]\n",
        "\n",
        "    # Try querying the victim model and handle potential API errors\n",
        "    try:\n",
        "        batch_embeddings = model_stealing(batch_imgs, PORT)\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Error for batch {i}: {e}\")\n",
        "        print(\"Sleeping 65 seconds before retrying batch...\")\n",
        "        time.sleep(65)  # Wait before retrying to avoid rate-limiting\n",
        "        continue\n",
        "\n",
        "    # Save the embeddings to a local file using pickle\n",
        "    with open(path, \"wb\") as f:\n",
        "        pickle.dump((list(range(start, end)), batch_embeddings), f)\n",
        "    print(f\"Saved batch {i} to {path}\")\n",
        "\n",
        "    # Delay between batches to prevent hitting the server too quickly\n",
        "    print(\"Waiting 65 seconds before next query to avoid rate limit...\")\n",
        "    time.sleep(65)"
      ],
      "metadata": {
        "id": "rP7u3z6Qpjje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training an Encoder to Mimic API Embedding"
      ],
      "metadata": {
        "id": "H1gVw0u99Okg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18\n",
        "import pickle, os\n",
        "\n",
        "# --- Custom Dataset class for images and target embeddings ---\n",
        "class DatasetWithTargets(Dataset):\n",
        "    def __init__(self, imgs, embeddings, transform=None):\n",
        "        self.imgs = imgs\n",
        "        self.embeddings = embeddings\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.transform(self.imgs[idx]) if self.transform else self.imgs[idx]\n",
        "        return img, torch.tensor(self.embeddings[idx], dtype=torch.float32)\n",
        "\n",
        "    def __len__(self): return len(self.imgs)\n",
        "\n",
        "# --- Modified ResNet18 to output 1024-dimensional embeddings ---\n",
        "class ResNet18Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        base = resnet18(weights=None)  # Load ResNet18 without pretrained weights\n",
        "        base.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)  # Modify first conv layer for smaller images\n",
        "        base.maxpool = nn.Identity()  # Remove max pooling to preserve spatial dimensions\n",
        "        self.backbone = nn.Sequential(*list(base.children())[:-1])  # Use all layers except the final FC\n",
        "        self.head = nn.Sequential(nn.Flatten(), nn.Linear(512, 1024))  # Project to 1024-dimensional space\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.head(self.backbone(x))\n",
        "\n",
        "# --- Hybrid Loss: Combines Cosine Similarity and L2 Distance ---\n",
        "class CosineL2Loss(nn.Module):\n",
        "    def __init__(self, alpha=0.5):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha  # Weight for cosine vs L2\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        cos_loss = 1 - F.cosine_similarity(pred, target, dim=1).mean()\n",
        "        l2_loss = F.mse_loss(pred, target)\n",
        "        return self.alpha * cos_loss + (1 - self.alpha) * l2_loss\n",
        "\n",
        "# --- Load images and their stolen embeddings from disk ---\n",
        "embedding_dir = \"embedding_batches\"\n",
        "dataset = torch.load(\"ModelStealingPub.pt\", weights_only=False)\n",
        "all_imgs, all_embeddings = [], []\n",
        "\n",
        "for file in sorted(os.listdir(embedding_dir)):\n",
        "    with open(os.path.join(embedding_dir, file), \"rb\") as f:\n",
        "        indices, embeddings = pickle.load(f)\n",
        "        all_imgs.extend([dataset.imgs[i] for i in indices])  # Retrieve original images\n",
        "        all_embeddings.extend(embeddings)  # Corresponding stolen embeddings\n",
        "\n",
        "print(f\"Loaded {len(all_imgs)} images and embeddings.\")\n",
        "\n",
        "# --- Data augmentation and normalization for training ---\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
        "    transforms.RandomGrayscale(p=0.1),\n",
        "    transforms.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
        "])\n",
        "\n",
        "# --- Create DataLoader for training ---\n",
        "train_dataset = DatasetWithTargets(all_imgs, all_embeddings, transform=train_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# --- Model, optimizer, and learning scheduler setup ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNet18Encoder().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.3)\n",
        "criterion = CosineL2Loss(alpha=0.5)  # Loss combining cosine and L2\n",
        "\n",
        "# --- Training loop ---\n",
        "model.train()\n",
        "for epoch in range(40):\n",
        "    total_loss = 0\n",
        "    for imgs, targets in train_loader:\n",
        "        imgs, targets = imgs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(imgs), targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * imgs.size(0)\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch+1}/40, Loss: {total_loss / len(train_loader.dataset):.4f}\")"
      ],
      "metadata": {
        "id": "BTCqEfq3pkij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exporting the Stolen Encoder to ONNX and Evaluating Its Accuracy"
      ],
      "metadata": {
        "id": "tyGXLQzy9-Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Export to ONNX ---\n",
        "dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
        "onnx_path = \"stolen_encoder.onnx\"\n",
        "torch.onnx.export(model, dummy_input, onnx_path,\n",
        "    input_names=[\"x\"], output_names=[\"output\"],\n",
        "    dynamic_axes={\"x\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
        "    export_params=True, opset_version=11)\n",
        "\n",
        "# --- ONNX Test ---\n",
        "try:\n",
        "    ort_session = ort.InferenceSession(onnx_path)\n",
        "    dummy_np = np.random.randn(1, 3, 32, 32).astype(np.float32)\n",
        "    onnx_out = ort_session.run(None, {\"x\": dummy_np})[0]\n",
        "    print(\"ONNX model output shape:\", onnx_out.shape)\n",
        "except Exception as e:\n",
        "    print(\"ONNX test failed:\", e)\n",
        "\n",
        "# --- Evaluate L2 Distance ---\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
        "])\n",
        "\n",
        "val_dataset = DatasetWithTargets(all_imgs, all_embeddings, transform=val_transforms)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    stolen_outputs = []\n",
        "    for imgs, _ in val_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        outputs = model(imgs).cpu().numpy()\n",
        "        stolen_outputs.append(outputs)\n",
        "\n",
        "    stolen_outputs = np.concatenate(stolen_outputs, axis=0)\n",
        "    victim_outputs = np.array(all_embeddings)\n",
        "\n",
        "    l2_distances = np.linalg.norm(victim_outputs - stolen_outputs, axis=1)\n",
        "    print(f\"\\n Raw Average L2 distance: {l2_distances.mean():.4f}\")\n",
        "    print(f\" Normalized L2 distance: {(l2_distances / np.linalg.norm(victim_outputs, axis=1)).mean():.4f}\")"
      ],
      "metadata": {
        "id": "q2Lq9QSZxuXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submitting the Model to the Server"
      ],
      "metadata": {
        "id": "mT9K7x6U-E6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Submit the ONNX model to the server ---\n",
        "headers = {\n",
        "    \"token\": TOKEN,\n",
        "    \"seed\": SEED,\n",
        "}\n",
        "\n",
        "try:\n",
        "    with open(onnx_path, \"rb\") as f:\n",
        "        response = requests.post(\n",
        "            \"http://34.122.51.94:9090/stealing\",\n",
        "            files={\"file\": f},\n",
        "            headers=headers\n",
        "        )\n",
        "        print(\"Submission status code:\", response.status_code)\n",
        "        print(\"Server response:\", response.text)\n",
        "except Exception as e:\n",
        "    print(\"Submission failed:\", e)"
      ],
      "metadata": {
        "id": "RQECftorhrBu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}